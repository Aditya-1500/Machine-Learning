{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wooden-dressing",
   "metadata": {},
   "source": [
    "# Character-level LSTM in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dying-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bearing-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "\n",
    "with open('data/anna.txt') as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modern-pitch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-welding",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "champion-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the text - map each character to an integer and vice versa\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch:ii for ii,ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infinite-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 47, 54, 11, 73, 41, 56, 35, 81, 63, 63, 63, 30, 54, 11, 11, 51,\n",
       "       35,  0, 54, 68, 77, 69, 77, 41,  2, 35, 54, 56, 41, 35, 54, 69, 69,\n",
       "       35, 54, 69, 77,  3, 41, 14, 35, 41, 79, 41, 56, 51, 35, 15, 10, 47,\n",
       "       54, 11, 11, 51, 35,  0, 54, 68, 77, 69, 51, 35, 77,  2, 35, 15, 10,\n",
       "       47, 54, 11, 11, 51, 35, 77, 10, 35, 77, 73,  2, 35, 46, 17, 10, 63,\n",
       "       17, 54, 51, 36, 63, 63, 80, 79, 41, 56, 51, 73, 47, 77, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-majority",
   "metadata": {},
   "source": [
    "### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "critical-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "def one_hot_encode(arr,n_labels):\n",
    "    \n",
    "    #Initialize the encoded array\n",
    "    one_hot = np.zeros((arr.size,n_labels),dtype=np.float32)\n",
    "    \n",
    "    #Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]),arr.flatten()] = 1\n",
    "    \n",
    "    #Reshape it to get to final one-hot encoded array\n",
    "    one_hot = one_hot.reshape((*arr.shape,n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "competitive-italy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Testing the one-hot encoding function\n",
    "test_seq = np.array([1,3,5,7])\n",
    "test_one_hot = one_hot_encode(test_seq,8)\n",
    "print(test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-testament",
   "metadata": {},
   "source": [
    "### Making training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sensitive-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    #Get the number of batches we can make\n",
    "    n_batches = arr.size // (batch_size*seq_length)\n",
    "    \n",
    "    #Total number of characters to keep from the array\n",
    "    arr = arr[:batch_size*seq_length*n_batches]\n",
    "    \n",
    "    #Reshape into batch_size rows\n",
    "    arr = arr.reshape(batch_size,-1)\n",
    "    \n",
    "    #Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0,arr.shape[1],seq_length):\n",
    "        #The features\n",
    "        x = arr[:,n:n+seq_length]\n",
    "        \n",
    "        #The targets\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1], y[:,-1] = x[:,1:], arr[:,n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:,:-1], y[:,-1] = x[:,1:], arr[:,0]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-endorsement",
   "metadata": {},
   "source": [
    "**Testing the implementation of above function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abandoned-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded,8,50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heard-occurrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[24 47 54 11 73 41 56 35 81 63]\n",
      " [ 2 46 10 35 73 47 54 73 35 54]\n",
      " [41 10  8 35 46 56 35 54 35  0]\n",
      " [ 2 35 73 47 41 35 37 47 77 41]\n",
      " [35  2 54 17 35 47 41 56 35 73]\n",
      " [37 15  2  2 77 46 10 35 54 10]\n",
      " [35 52 10 10 54 35 47 54  8 35]\n",
      " [18 42 69 46 10  2  3 51 36 35]]\n",
      "y\n",
      " [[47 54 11 73 41 56 35 81 63 63]\n",
      " [46 10 35 73 47 54 73 35 54 73]\n",
      " [10  8 35 46 56 35 54 35  0 46]\n",
      " [35 73 47 41 35 37 47 77 41  0]\n",
      " [ 2 54 17 35 47 41 56 35 73 41]\n",
      " [15  2  2 77 46 10 35 54 10  8]\n",
      " [52 10 10 54 35 47 54  8 35  2]\n",
      " [42 69 46 10  2  3 51 36 35 25]]\n"
     ]
    }
   ],
   "source": [
    "#Printing the first 10 items in a sequence\n",
    "print('x\\n',x[:10,:10])\n",
    "print('y\\n',y[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-external",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "helpful-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU...\n"
     ]
    }
   ],
   "source": [
    "#Checking if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"Training on GPU...\")\n",
    "else:\n",
    "    print(\"GPU not available..Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "published-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        #Creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii,ch in self.int2char.items()}\n",
    "        \n",
    "        #Defining the layers of the model\n",
    "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=self.n_hidden, num_layers=self.n_layers,\n",
    "                            batch_first=True, dropout=self.drop_prob)\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        self.fc = nn.Linear(self.n_hidden,len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        '''\n",
    "        Forward pass through the network.\n",
    "        These inputs are x, and the hidden state/cell state `hidden`.\n",
    "        '''\n",
    "        \n",
    "        #Get the outputs and new hidden state from the LSTM\n",
    "        r_output, hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        #Pass the output through dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        #Stack up LSTM outputs using view\n",
    "        out = out.contiguous().view(-1,self.n_hidden)\n",
    "        \n",
    "        #Finally pass the output through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out,hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        ''' Initializes the hidden state '''\n",
    "        #Create two new tensors with sizes n_layers x batch_size x n_hidden \n",
    "        #initialized to zero, for hidden state and cell state for LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers,batch_size,self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers,batch_size,self.n_hidden).zero_())\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-onion",
   "metadata": {},
   "source": [
    "## Defining the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "combined-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    '''\n",
    "    Training a network\n",
    "    Arguments\n",
    "    ----------\n",
    "    net: CharRNN Network\n",
    "    data: text data to train our network\n",
    "    epochs: Number of epochs to train\n",
    "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "    seq_length: Number of character steps per mini-batch\n",
    "    lr: learning rate\n",
    "    clip: gradient clipping\n",
    "    val_frac: Fraction of data to hold out for validation\n",
    "    print_every: Number of steps for printing training and validation loss\n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Creating training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        #Initialize the hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x,y in get_batches(data,batch_size,seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            #One-hot encoding our data to feed into network\n",
    "            x = one_hot_encode(x,n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            #Creating new variables for hidden state\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            \n",
    "            #Getting output from the model\n",
    "            output, h = net(inputs,h)\n",
    "            \n",
    "            #Calculating the loss and performing backpropagation\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Loss stats\n",
    "            if counter%print_every == 0:\n",
    "                #Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                \n",
    "                for x,y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x,n_chars)\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    if train_on_gpu:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    output, val_h = net(inputs,val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1,epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-individual",
   "metadata": {},
   "source": [
    "### Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "treated-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Setting the model hyperparameters\n",
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "\n",
    "net = CharRNN(chars,n_hidden,n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caring-frank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2670... Val Loss: 3.2083\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1565... Val Loss: 3.1376\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1504... Val Loss: 3.1250\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1168... Val Loss: 3.1195\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1422... Val Loss: 3.1180\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1212... Val Loss: 3.1161\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1088... Val Loss: 3.1156\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1233... Val Loss: 3.1132\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1252... Val Loss: 3.1088\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1095... Val Loss: 3.1000\n",
      "Epoch: 1/20... Step: 110... Loss: 3.0934... Val Loss: 3.0786\n",
      "Epoch: 1/20... Step: 120... Loss: 3.0337... Val Loss: 3.0314\n",
      "Epoch: 1/20... Step: 130... Loss: 3.0193... Val Loss: 2.9601\n",
      "Epoch: 2/20... Step: 140... Loss: 2.9199... Val Loss: 2.8837\n",
      "Epoch: 2/20... Step: 150... Loss: 2.8279... Val Loss: 2.8028\n",
      "Epoch: 2/20... Step: 160... Loss: 2.7573... Val Loss: 2.7042\n",
      "Epoch: 2/20... Step: 170... Loss: 2.6394... Val Loss: 2.6574\n",
      "Epoch: 2/20... Step: 180... Loss: 2.5814... Val Loss: 2.5516\n",
      "Epoch: 2/20... Step: 190... Loss: 2.5273... Val Loss: 2.5066\n",
      "Epoch: 2/20... Step: 200... Loss: 2.5131... Val Loss: 2.4741\n",
      "Epoch: 2/20... Step: 210... Loss: 2.4721... Val Loss: 2.4379\n",
      "Epoch: 2/20... Step: 220... Loss: 2.4287... Val Loss: 2.4002\n",
      "Epoch: 2/20... Step: 230... Loss: 2.4157... Val Loss: 2.3711\n",
      "Epoch: 2/20... Step: 240... Loss: 2.3970... Val Loss: 2.3533\n",
      "Epoch: 2/20... Step: 250... Loss: 2.3369... Val Loss: 2.3271\n",
      "Epoch: 2/20... Step: 260... Loss: 2.3194... Val Loss: 2.3032\n",
      "Epoch: 2/20... Step: 270... Loss: 2.3109... Val Loss: 2.2841\n",
      "Epoch: 3/20... Step: 280... Loss: 2.3160... Val Loss: 2.2596\n",
      "Epoch: 3/20... Step: 290... Loss: 2.2717... Val Loss: 2.2333\n",
      "Epoch: 3/20... Step: 300... Loss: 2.2513... Val Loss: 2.2129\n",
      "Epoch: 3/20... Step: 310... Loss: 2.2297... Val Loss: 2.1963\n",
      "Epoch: 3/20... Step: 320... Loss: 2.2017... Val Loss: 2.1698\n",
      "Epoch: 3/20... Step: 330... Loss: 2.1620... Val Loss: 2.1543\n",
      "Epoch: 3/20... Step: 340... Loss: 2.1792... Val Loss: 2.1360\n",
      "Epoch: 3/20... Step: 350... Loss: 2.1679... Val Loss: 2.1158\n",
      "Epoch: 3/20... Step: 360... Loss: 2.0960... Val Loss: 2.0949\n",
      "Epoch: 3/20... Step: 370... Loss: 2.1162... Val Loss: 2.0780\n",
      "Epoch: 3/20... Step: 380... Loss: 2.0952... Val Loss: 2.0617\n",
      "Epoch: 3/20... Step: 390... Loss: 2.0713... Val Loss: 2.0472\n",
      "Epoch: 3/20... Step: 400... Loss: 2.0379... Val Loss: 2.0325\n",
      "Epoch: 3/20... Step: 410... Loss: 2.0448... Val Loss: 2.0136\n",
      "Epoch: 4/20... Step: 420... Loss: 2.0306... Val Loss: 1.9977\n",
      "Epoch: 4/20... Step: 430... Loss: 2.0210... Val Loss: 1.9816\n",
      "Epoch: 4/20... Step: 440... Loss: 2.0038... Val Loss: 1.9772\n",
      "Epoch: 4/20... Step: 450... Loss: 1.9438... Val Loss: 1.9523\n",
      "Epoch: 4/20... Step: 460... Loss: 1.9385... Val Loss: 1.9382\n",
      "Epoch: 4/20... Step: 470... Loss: 1.9621... Val Loss: 1.9248\n",
      "Epoch: 4/20... Step: 480... Loss: 1.9393... Val Loss: 1.9140\n",
      "Epoch: 4/20... Step: 490... Loss: 1.9454... Val Loss: 1.8995\n",
      "Epoch: 4/20... Step: 500... Loss: 1.9342... Val Loss: 1.8859\n",
      "Epoch: 4/20... Step: 510... Loss: 1.9122... Val Loss: 1.8732\n",
      "Epoch: 4/20... Step: 520... Loss: 1.9282... Val Loss: 1.8676\n",
      "Epoch: 4/20... Step: 530... Loss: 1.8779... Val Loss: 1.8550\n",
      "Epoch: 4/20... Step: 540... Loss: 1.8412... Val Loss: 1.8433\n",
      "Epoch: 4/20... Step: 550... Loss: 1.8877... Val Loss: 1.8300\n",
      "Epoch: 5/20... Step: 560... Loss: 1.8504... Val Loss: 1.8217\n",
      "Epoch: 5/20... Step: 570... Loss: 1.8361... Val Loss: 1.8113\n",
      "Epoch: 5/20... Step: 580... Loss: 1.8283... Val Loss: 1.8009\n",
      "Epoch: 5/20... Step: 590... Loss: 1.8097... Val Loss: 1.7881\n",
      "Epoch: 5/20... Step: 600... Loss: 1.8076... Val Loss: 1.7845\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7905... Val Loss: 1.7758\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7916... Val Loss: 1.7695\n",
      "Epoch: 5/20... Step: 630... Loss: 1.8002... Val Loss: 1.7534\n",
      "Epoch: 5/20... Step: 640... Loss: 1.7703... Val Loss: 1.7464\n",
      "Epoch: 5/20... Step: 650... Loss: 1.7540... Val Loss: 1.7385\n",
      "Epoch: 5/20... Step: 660... Loss: 1.7354... Val Loss: 1.7294\n",
      "Epoch: 5/20... Step: 670... Loss: 1.7625... Val Loss: 1.7259\n",
      "Epoch: 5/20... Step: 680... Loss: 1.7551... Val Loss: 1.7161\n",
      "Epoch: 5/20... Step: 690... Loss: 1.7284... Val Loss: 1.7125\n",
      "Epoch: 6/20... Step: 700... Loss: 1.7251... Val Loss: 1.7056\n",
      "Epoch: 6/20... Step: 710... Loss: 1.7141... Val Loss: 1.6968\n",
      "Epoch: 6/20... Step: 720... Loss: 1.7086... Val Loss: 1.6919\n",
      "Epoch: 6/20... Step: 730... Loss: 1.7228... Val Loss: 1.6836\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6853... Val Loss: 1.6757\n",
      "Epoch: 6/20... Step: 750... Loss: 1.6666... Val Loss: 1.6718\n",
      "Epoch: 6/20... Step: 760... Loss: 1.7034... Val Loss: 1.6641\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6866... Val Loss: 1.6591\n",
      "Epoch: 6/20... Step: 780... Loss: 1.6754... Val Loss: 1.6502\n",
      "Epoch: 6/20... Step: 790... Loss: 1.6507... Val Loss: 1.6462\n",
      "Epoch: 6/20... Step: 800... Loss: 1.6666... Val Loss: 1.6402\n",
      "Epoch: 6/20... Step: 810... Loss: 1.6666... Val Loss: 1.6354\n",
      "Epoch: 6/20... Step: 820... Loss: 1.6160... Val Loss: 1.6282\n",
      "Epoch: 6/20... Step: 830... Loss: 1.6586... Val Loss: 1.6246\n",
      "Epoch: 7/20... Step: 840... Loss: 1.6154... Val Loss: 1.6219\n",
      "Epoch: 7/20... Step: 850... Loss: 1.6316... Val Loss: 1.6149\n",
      "Epoch: 7/20... Step: 860... Loss: 1.6149... Val Loss: 1.6115\n",
      "Epoch: 7/20... Step: 870... Loss: 1.6277... Val Loss: 1.6048\n",
      "Epoch: 7/20... Step: 880... Loss: 1.6262... Val Loss: 1.6006\n",
      "Epoch: 7/20... Step: 890... Loss: 1.6218... Val Loss: 1.5947\n",
      "Epoch: 7/20... Step: 900... Loss: 1.6010... Val Loss: 1.5904\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5769... Val Loss: 1.5899\n",
      "Epoch: 7/20... Step: 920... Loss: 1.6015... Val Loss: 1.5841\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5793... Val Loss: 1.5794\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5858... Val Loss: 1.5754\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5942... Val Loss: 1.5722\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5991... Val Loss: 1.5684\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5940... Val Loss: 1.5639\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5764... Val Loss: 1.5612\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5706... Val Loss: 1.5558\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5601... Val Loss: 1.5492\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5950... Val Loss: 1.5449\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5658... Val Loss: 1.5438\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.5468... Val Loss: 1.5378\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.5632... Val Loss: 1.5379\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.5444... Val Loss: 1.5338\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.5475... Val Loss: 1.5288\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.5400... Val Loss: 1.5294\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.5410... Val Loss: 1.5238\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.5304... Val Loss: 1.5209\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.5197... Val Loss: 1.5168\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.5130... Val Loss: 1.5102\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.5374... Val Loss: 1.5120\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.5254... Val Loss: 1.5093\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.5211... Val Loss: 1.5044\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.5438... Val Loss: 1.5044\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4955... Val Loss: 1.4982\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.5037... Val Loss: 1.4951\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4999... Val Loss: 1.4939\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.5357... Val Loss: 1.4911\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4768... Val Loss: 1.4862\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4870... Val Loss: 1.4848\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4913... Val Loss: 1.4824\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4700... Val Loss: 1.4779\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4701... Val Loss: 1.4771\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4875... Val Loss: 1.4746\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4881... Val Loss: 1.4718\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4801... Val Loss: 1.4703\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4804... Val Loss: 1.4675\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4740... Val Loss: 1.4675\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4629... Val Loss: 1.4615\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4779... Val Loss: 1.4597\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4492... Val Loss: 1.4603\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4427... Val Loss: 1.4578\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.4374... Val Loss: 1.4546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1350... Loss: 1.4321... Val Loss: 1.4530\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.4274... Val Loss: 1.4540\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.4284... Val Loss: 1.4506\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4670... Val Loss: 1.4450\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4625... Val Loss: 1.4472\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4750... Val Loss: 1.4467\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4805... Val Loss: 1.4431\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4697... Val Loss: 1.4359\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.4367... Val Loss: 1.4379\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4639... Val Loss: 1.4330\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3875... Val Loss: 1.4320\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.4059... Val Loss: 1.4319\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.4028... Val Loss: 1.4290\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.4238... Val Loss: 1.4260\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.4180... Val Loss: 1.4273\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.4058... Val Loss: 1.4257\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3873... Val Loss: 1.4217\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.4223... Val Loss: 1.4210\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4707... Val Loss: 1.4222\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.4342... Val Loss: 1.4207\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.4390... Val Loss: 1.4183\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.4464... Val Loss: 1.4152\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3806... Val Loss: 1.4120\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3633... Val Loss: 1.4102\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3650... Val Loss: 1.4087\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3897... Val Loss: 1.4121\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3871... Val Loss: 1.4070\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3780... Val Loss: 1.4038\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.4042... Val Loss: 1.4031\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3759... Val Loss: 1.4017\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3528... Val Loss: 1.4046\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.4093... Val Loss: 1.4018\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3822... Val Loss: 1.4056\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3962... Val Loss: 1.4003\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3726... Val Loss: 1.3969\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3728... Val Loss: 1.3931\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3560... Val Loss: 1.3922\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3549... Val Loss: 1.3928\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3972... Val Loss: 1.3901\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3642... Val Loss: 1.3908\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3312... Val Loss: 1.3918\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3532... Val Loss: 1.3859\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3778... Val Loss: 1.3877\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3509... Val Loss: 1.3863\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3365... Val Loss: 1.3818\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3744... Val Loss: 1.3850\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3672... Val Loss: 1.3865\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3480... Val Loss: 1.3828\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3736... Val Loss: 1.3792\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.3225... Val Loss: 1.3787\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2978... Val Loss: 1.3781\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3505... Val Loss: 1.3771\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3649... Val Loss: 1.3733\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3590... Val Loss: 1.3771\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3869... Val Loss: 1.3747\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3499... Val Loss: 1.3713\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3541... Val Loss: 1.3709\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3475... Val Loss: 1.3709\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.3085... Val Loss: 1.3664\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3719... Val Loss: 1.3698\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3347... Val Loss: 1.3731\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3334... Val Loss: 1.3655\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.3312... Val Loss: 1.3621\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.3190... Val Loss: 1.3657\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.3207... Val Loss: 1.3615\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.3018... Val Loss: 1.3618\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.3216... Val Loss: 1.3597\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3531... Val Loss: 1.3616\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.3113... Val Loss: 1.3592\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.3189... Val Loss: 1.3563\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.3141... Val Loss: 1.3556\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.3238... Val Loss: 1.3563\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.3375... Val Loss: 1.3533\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.3265... Val Loss: 1.3561\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.3297... Val Loss: 1.3577\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.3077... Val Loss: 1.3527\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.3063... Val Loss: 1.3511\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.3164... Val Loss: 1.3509\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2967... Val Loss: 1.3487\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.3039... Val Loss: 1.3487\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.3262... Val Loss: 1.3448\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.3043... Val Loss: 1.3520\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.3074... Val Loss: 1.3474\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2995... Val Loss: 1.3446\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.3179... Val Loss: 1.3453\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2954... Val Loss: 1.3425\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2593... Val Loss: 1.3407\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.3196... Val Loss: 1.3434\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2818... Val Loss: 1.3417\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2975... Val Loss: 1.3400\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2865... Val Loss: 1.3387\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2915... Val Loss: 1.3385\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.3006... Val Loss: 1.3371\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.3006... Val Loss: 1.3348\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.3055... Val Loss: 1.3381\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2655... Val Loss: 1.3370\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2941... Val Loss: 1.3338\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2729... Val Loss: 1.3339\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2780... Val Loss: 1.3333\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2990... Val Loss: 1.3318\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2874... Val Loss: 1.3292\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2971... Val Loss: 1.3331\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2634... Val Loss: 1.3357\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2820... Val Loss: 1.3343\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2889... Val Loss: 1.3326\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2973... Val Loss: 1.3287\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2971... Val Loss: 1.3261\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2780... Val Loss: 1.3242\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2953... Val Loss: 1.3288\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2637... Val Loss: 1.3265\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2621... Val Loss: 1.3236\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2730... Val Loss: 1.3266\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2742... Val Loss: 1.3236\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2641... Val Loss: 1.3246\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2605... Val Loss: 1.3216\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2633... Val Loss: 1.3253\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2704... Val Loss: 1.3235\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2790... Val Loss: 1.3235\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2833... Val Loss: 1.3212\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2950... Val Loss: 1.3210\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2660... Val Loss: 1.3199\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2735... Val Loss: 1.3180\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2620... Val Loss: 1.3176\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2952... Val Loss: 1.3150\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2471... Val Loss: 1.3166\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2510... Val Loss: 1.3205\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2523... Val Loss: 1.3187\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2441... Val Loss: 1.3125\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2551... Val Loss: 1.3154\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2684... Val Loss: 1.3180\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2694... Val Loss: 1.3167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 2660... Loss: 1.2646... Val Loss: 1.3187\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2773... Val Loss: 1.3161\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2712... Val Loss: 1.3122\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2551... Val Loss: 1.3127\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2776... Val Loss: 1.3075\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.2353... Val Loss: 1.3109\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2305... Val Loss: 1.3135\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.2332... Val Loss: 1.3097\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.2333... Val Loss: 1.3116\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2425... Val Loss: 1.3108\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.2299... Val Loss: 1.3074\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2702... Val Loss: 1.3058\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2870... Val Loss: 1.3042\n"
     ]
    }
   ],
   "source": [
    "#Setting the training hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-bailey",
   "metadata": {},
   "source": [
    "### Saving checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cross-income",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7045cec6e8c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'char_rnn_1.net'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m checkpoint = {'n_hidden':net.n_hidden,\n\u001b[0m\u001b[0;32m      4\u001b[0m               \u001b[1;34m'n_layers'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               \u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = 'char_rnn_1.net'\n",
    "\n",
    "checkpoint = {'n_hidden':net.n_hidden,\n",
    "              'n_layers':net.n_layers,\n",
    "              'state_dict':net.state_dict(),\n",
    "              'tokens':net.chars}\n",
    "\n",
    "with open(model_name,'wb') as f:\n",
    "    torch.save(checkpoint,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-exception",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "royal-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "    '''\n",
    "    Given a character, predict the next character\n",
    "    Returns the predicted character and hidden state.\n",
    "    '''\n",
    "    \n",
    "    #Tensor inputs\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x,len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "    #Detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    #Get the output from the model\n",
    "    out, h = net(inputs,h)\n",
    "    \n",
    "    #Get the character probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    if train_on_gpu:\n",
    "        p = p.cpu()\n",
    "    \n",
    "    #Get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "    #Selecting the most likely character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch,p=p/p.sum())\n",
    "    \n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-knowing",
   "metadata": {},
   "source": [
    "### Priming and generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enhanced-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime=\"The\", top_k=None):\n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    #Firstly run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "        \n",
    "    chars.append(char)\n",
    "    \n",
    "    for ii in range(size):\n",
    "        char,h = predict(net,chars[-1],h,top_k=top_k)\n",
    "        chars.append(char)\n",
    "    \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "billion-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robing, at the delicate stirr, and the dates was a church at the\n",
      "subject, the man and the conversation with\n",
      "hands with a change of weary at\n",
      "the carry of his faily and song. A blindering were a man of hands of the\n",
      "manship.\n",
      "\n",
      "\"I can, thank you to be in him? Tometime that I'm a considering that it's a man who\n",
      "say one to be in the same arrish, with the soun, of we may tell you his\n",
      "sense, that's too, we shall seem it?\" he said, smiling, \"but I was a condition, I didn't believe that you would not\n",
      "speak\n",
      "into the crubled friends in their pincers, they seem to start. But the children and woman\n",
      "was new table, said in such a still. Well, and so, as though if you are such all the marshal. In horrible sister--her son's\n",
      "chance with\n",
      "the menthal of the\n",
      "same, when you see her hat in\n",
      "such thing,\n",
      "but\n",
      "the choose of them, we have been the plass, the fact with shame of hands.\"\n",
      "\n",
      "Sveazhsky did not know what was in the softing, he showed them one of the condition of the money. And the\n",
      "same time.\n",
      "\n",
      "\n",
      "\n",
      "Chapter 12\n",
      "\n",
      "\n",
      "She\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "print(sample(net,1000,\"Robin\",top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-garden",
   "metadata": {},
   "source": [
    "### Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "careful-glance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we are loading a pre-trained model 'char_rnn_1.net' that trained on 20 epochs\n",
    "with open('char_rnn_1.net','rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "loaded_model = CharRNN(checkpoint['tokens'],checkpoint['n_hidden'],checkpoint['n_layers'])\n",
    "loaded_model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "editorial-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunil and talk to her, as though he\n",
      "sear his heed badd of all of the\n",
      "chingles to her. And it was something, and\n",
      "he had anyone to tell the mother's, took it.\n",
      "\n",
      "Alexey Alexandrovitch had night as the same time to say to her, as had something hopely began to go\n",
      "to be anything with husbands of him the same partical end\n",
      "of hands and\n",
      "carriage, having been\n",
      "anger for the painted at the stands of her head. He wished, and\n",
      "straight into her faility. The dress she saw that she was never to be the\n",
      "subject of the servants of the same. Alexey Alexandrovitch answered the country of the party. Sergey\n",
      "Ivanovitch, which was in her eyes oncome her second, taking her head, and showing her through harmly friend, and there was no chair\n",
      "and saying a look on his began from that, always had been said, sitting on what was nothing and stood far and heard that she could not help his soul about that, had not taken him all. She went on.\n",
      "\n",
      "\"I am not ask her for the carriage,\" she said, smiling, \"where so you wants to see you on her families and my study,\" said Vronsky, who saw\n",
      "than anything where he had been branning away. \"I am not a good brought on, I was anything...\"\n",
      "\n",
      "\"Yashvin was to be the stop, I don't believe that,\" he said to hardly at the missable, and that there was all that weared and all the study, the proport was to brought him and short from his wife. \"And I was taken the same, to say, and too! Alexey I've been at the\n",
      "master to those and men that the man of me, I caunting.\n",
      "Anna said that the more and that insice house would be as he will see her, who has almortled is a late and\n",
      "district, what was the side of the motion, where you did\n",
      "not go on to tell you here as the story to be\n",
      "thinking of them, to be a sigries in the\n",
      "could not be soled of anything, that to think of\n",
      "his figure.\"\n",
      "\n",
      "Stepan Arkadyevitch and something and his watch and said\n",
      "that she was simple to the proper of the crimical, and this steps with a presence and\n",
      "strange in the\n",
      "moment that she did not know that, that she could not come\n",
      "out, and shate on the feoling he heard herself and his habit, but to be in hearing to his hand, and to say the children, he felt to take him the peasant that he cauled bess had been served on in the door, as he saw that how it would be too, took on how to take her face. Sergey Ivanovitch could never care for him all the dividion, and all of the same\n",
      "crowd to\n",
      "the same, and the second pleasant carriage he was thought\n",
      "of an offer with the\n",
      "sons, who was to\n",
      "converse it.\n",
      "\n",
      "\"What do you see, I suppose.\"\n",
      "\n",
      "\"Yes, I do\n",
      "something im of an one in that.\"\n",
      "\n",
      "\"And there is a passionare of my father.\"\n",
      "\n",
      "\"What is it you talk. A better milk,\" he said, stepping him before her.\n",
      "\n",
      "\"I have a seligity,\" he said. \"Well, that's\n",
      "true too, and Indoutabtery that there was not always to see a constinution of such suffering in his wail,\" said Vassenka stairingly. That in which he was not taken off the sound of her husband's simple and the mother\n",
      "talking at once, there was the\n",
      "conversation with the carpenters, as though \n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded_model,3000,top_k=5,prime=\"Sunil \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "plastic-insurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something, too had to say thas it seemed, between, and the charming started oneserf, to the princess and her side, which should\n",
      "be still to be said. At took him with a stand of hand,\n",
      "and the same time they was something and talked to the station to the son was a care to talk\n",
      "about him as they\n",
      "could be a lattle overcome and her father. He was so much that he had thought that in his face as he saw all at once to\n",
      "say about the cause, that he had not been a long while, and sating a carriage, and his state in a character of her heart.\n",
      "\n",
      "\"What, that you want to say it to tell me, and I will go over horror of that man that they were a present discassion, then they has so much a state on to see him.\"\n",
      "\n",
      "\"Yes, but I'll stand on anyone. We have been told you.\" And he went to stop things.\n",
      "\n",
      "\"What do you\n",
      "thought they want to see her,\" said Levin. \"I'm not\n",
      "a children. And there was a committee.\"\n",
      "\n",
      "And the sound of his study and still seeing the carriage,\n",
      "and she would never have been a single smoother.\n",
      "\n",
      "And so that the princess said to his state. He was saying. They were so that he went over the same stare and struck and countes, but he saw that there was an instant of his sister, and the parting hands of his family as to him. He\n",
      "had been started, and that they\n",
      "were always forgivened to a conversation of her family, hiden he had so still\n",
      "then at the chair. They heard him and his face to him to have to\n",
      "be took home,\n",
      "but he saw the standing at the councre, he had been to see her, but he had\n",
      "not talking to\n",
      "him.\n",
      "\n",
      "The carriage was so many\n",
      "sound.\n",
      "\n",
      "\"You don't know the chair, and I should be to stay of them, but I can't be as that I can't talk about you,\" he said, and the proposing one of the portrait with him.\n",
      "\n",
      "There he was not to\n",
      "be taken to her at the sone, as though there was the sacidul to the conversation.\n",
      "\n",
      "\"Well thinks,\" she thought and happened, and he shalled his face. \"I don't want to be\n",
      "sorry, and I shall be a long again,\" said Stepan Arkadyevitch, with an entrance outside all his\n",
      "face, but the carriage was something watching her head. \"When, I don't know the sort of the\n",
      "court of the children,\" added Alexey Alexandrovitch.\n",
      "\n",
      "\"Well, this is that yet I certainly do it,\" said Alexey Alexandrovitch.\n",
      "\n",
      "\"I have no man so talking, but I say that I shale to go to the\n",
      "same to me, and you see, and we'll give him it all one of this.\"\n",
      "\n",
      "\"Well,\n",
      "this is the same an old man, too, I can't stop the same and something in the sounds of the\n",
      "cape and so much. I'm always did\n",
      "there.\"\n",
      "\n",
      "\"I have told me and the past of the\n",
      "party of the country,\" said Anna, which\n",
      "had not to go\n",
      "on.\n",
      "\n",
      "\"What are you thinking of this thing I can't\n",
      "believe it,\" she talked, and standing his feelings at\n",
      "a light, and she was so still and\n",
      "straight to the stream of\n",
      "taking one of his heart. There was a station with the carriage to his single, and the states of the conviction of their heart he was too and the same soul of her father, and\n",
      "she could not be saying the prince, starting to her fore that this seeming of this\n",
      "moment, and she had been a little cheek, which\n",
      "had been sorry for her. The people, who had not seen the contrary of the same to see her and had been stinging and called his heart was to stay of the\n",
      "possibility of the prettiness, and had breathed the soul and start of a strangered to the coming to the satisfaction, and his house of a conversation of the paint with a companion of their conversation when\n",
      "he saw the\n",
      "princess to him. He\n",
      "stopped, and still struck\n",
      "at all the propontior of\n",
      "his face and happy than that, all that had been already said:\n",
      " \"Well where is it you see that you've been so much meant out of all the counting herse to the same. What is there was to\n",
      "be to be taken to the conversation to me, I can do the part of her as the same time in the conducion to think of\n",
      "his heart and say it. I am not so talking to the council and them, and I\n",
      "chied a long thing and an old particuler and the\n",
      "princess, and what has been too, and I don't know what is in their moments...\"\n",
      "\n",
      "\"I'm always been asked it and seemed. What is it you did not know what you are to be a single of marriad, and\n",
      "I can't believe that you say that I can to see the strange of the\n",
      "parties with your family, and I'm a simple asked you to be saying that there are, to\n",
      "make him to see how, the marriage, the children went out of the contrary. I have so much taking of the courity to be\n",
      "some peace and that, and when they was not the standing, and I should be a love, and there was something would have been a sight of the principles, and who can to get out of the portrait.\"\n",
      "\n",
      "\"I don't know when they're the sounds of the station of the children...\"\n",
      "\n",
      "\"I am not\n",
      "so more. We've been\n",
      "said to the same torture... That I'm not as it would be a service in the children and the soul, this said it were all the painter, and to me that this moments were\n",
      "somiting all as it was a sent of many siggisite and\n",
      "the sace to my words. I denighted the province over her husband.\n",
      "\n",
      "\"Well, was it that I don't know,\" he said, with a smile of her straigh\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded_model,5000,top_k=3,prime=\"Something\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "capable-marijuana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He said a moment, began walking out of that things, and straight in a smile. The strange there was a men in\n",
      "his subject at the more and complexe of, though anyone, and taking off himself.\n",
      "She were so saying that\n",
      "it's so such as impossible to see them, brandy them to successful\n",
      "into the crowd,\n",
      "and there was to sudden the pasence, there are a look of\n",
      "hatitures that he had as though shill and have been to difficult first the middle of a man with\n",
      "her, and,\" she said to her,\n",
      "said sometimes.\n",
      "\n",
      "Stepan Arkadyevitch, and as he did not talk to herself, with which, he was said as she was going, but to be delighted it. He was so say that he had an helper on his horses and the man whether the courses of weaking of the\n",
      "mother, he had nithtring what seemed in\n",
      "a straight that had been a straight of\n",
      "his chatter of his brother's\n",
      "cases of three things of her country.\n",
      "She was since\n",
      "the corder that with simply carriage.\n",
      "\n",
      "\"I am the friend and son's so much more friend of all to her.\"\n",
      "\n",
      "\"Ah! why do you\n",
      "mean? Who has been at the sound of the crushions of the same torture, where in the\n",
      "part of the subject, but the chearers of it, and they surdon the man so sort, and this, I was grasping of anything both of\n",
      "mattary.\"\n",
      "\n",
      "Stepan Arkadyevitch\n",
      "was still\n",
      "trying to say, and had been the cream of the place where to say so make off to himself, to be in three simple over ones who was the carreate to things to his house which had seared it, and he saw his word. \"That does I can't stay to that thing.....\"\n",
      "\n",
      "The doctor had been sawing a subject, still means of the country. And the district with the conversation was the sore of talk, and he went out, told him to the corner of tea and tark of his head, but that the soluriaging somethirg he could not tell him\n",
      "that she was a sing to the change and her strain, and stood her friend, and his secondly true the saces, and she had not supposed to him and a porchation and to grief over him.\n",
      "\"It was all the princistic smile. I'm going to here. I'm sorry and who was a more state \n"
     ]
    }
   ],
   "source": [
    "#Sample using loaded model\n",
    "print(sample(loaded_model,2000,top_k=5,prime=\"He said \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-kansas",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
